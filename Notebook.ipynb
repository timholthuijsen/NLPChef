{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining Group Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative idea: maybe make model input a list of ingredients (based on what you have at home), and then let the model generate a reicpe for you based on the ingredients you have available\n",
    "\n",
    "if that is the route we take, we can alternatively make another function that generates random lists of ingredients, and then feed the random ingredients to the recipe maker based on ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#https://eightportions.com/datasets/Recipes/#fn:1 where the data is from\n",
    "#scraped from \n",
    "#Foodnetwork.com, Epicurious.com, Allrecipes.com by Ryan Lee\n",
    "\n",
    "allr = open('recipes_raw_nosource_ar.json')\n",
    "epi = open('recipes_raw_nosource_epi.json')\n",
    "food = open('recipes_raw_nosource_fn.json')\n",
    "\n",
    "data1 = json.load(allr) #data 1 has a lot of the word ADVERTISEMENT\n",
    "data2 = json.load(epi) #data 2 looks good\n",
    "data3 = json.load(food) #this too\n",
    "\n",
    "#this load module loads the data into a dictionary \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammie Hamblet's Deviled Crab\n"
     ]
    }
   ],
   "source": [
    "#Example of the double dict structue of the .json files:\n",
    "print(data3[\"p3pKOD6jIHEcjf20CCXohP8uqkG5dGi\"]['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some help from [coursera](https://www.coursera.org/projects/generating-new-recipes-python)\n",
    "\n",
    "Our current recipes are in a dict of list of dict form, where the first dict contains the recipe code as key and the recipe as value, and the recipe value itself is a list of dicts in the shape [{title: value}, {ingredients: value}, {instructions:value}]. This needs to be processed to be more workable\n",
    "\n",
    "\n",
    "We start by defining a list of all the recipe codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first dataset contains 39802 recipes\n",
      "in total, we have 125164 recipes\n"
     ]
    }
   ],
   "source": [
    "#creating lists of keys\n",
    "codes1 = list(data1.keys())\n",
    "codes2 = list(data2.keys())\n",
    "codes3 = list(data3.keys())\n",
    "\n",
    "#and doing some data exploration\n",
    "NumRecipes = len(codes1) + len(codes2) + len(codes3)\n",
    "print('The first dataset contains', len(codes1), 'recipes')\n",
    "print('in total, we have', NumRecipes, 'recipes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Pandas dataframe\n",
    "The dict of lists of dicts format is rather annoying to work with for the modeling tasks we want to perform on it. Therefore, we convert everything nicely into our own pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data has been added to the lists succesfully!\n"
     ]
    }
   ],
   "source": [
    "#Create dataframe\n",
    "Data = pd.DataFrame()\n",
    "\n",
    "#initializing empty lists which we'll add to the dataframe\n",
    "Title = []\n",
    "Ingredients = []\n",
    "Instructions = []\n",
    "\n",
    "#a for-loop to put all the required data in the lists\n",
    "datasets = [data1, data2, data3]\n",
    "\n",
    "for data in datasets:\n",
    "    for _, val in data.items():\n",
    "        #We occasionally get keyerrors due to corrupted data\n",
    "        #so a try-except is added\n",
    "        try:\n",
    "            Title.append(val['title'])\n",
    "            Ingredients.append([\n",
    "                #And we remove the random ADVERTISEMENT clutter\n",
    "                ingredient.replace(\n",
    "                'ADVERTISEMENT', '') for ingredient in val['ingredients']])\n",
    "            Instructions.append([str(\n",
    "                val['instructions']).replace('ADVERTISEMENT','').replace('\\n', ' ')])\n",
    "                              \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "#Quick check to see if it worked\n",
    "if len(Title) == len(Ingredients) and len(Title) == len(Instructions):\n",
    "    print(\"All data has been added to the lists succesfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During this transformation, 517 empty values have been removed\n",
      "We now have 124647 recipes\n"
     ]
    }
   ],
   "source": [
    "print(\"During this transformation,\", NumRecipes - len(Title), \"empty values have been removed\")\n",
    "print(\"We now have\", len(Title), \"recipes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier, we noticed that the first dataset contained a lot of random ADVERTISEMENT strings scattered around. This clutter has also been removed during the list comprehensions used above\n",
    "#### Adding data to dataframe\n",
    "We can now add all of the data we just created and finish up the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Ingredients</th>\n",
       "      <th>Instructions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Slow Cooker Chicken and Dumplings</td>\n",
       "      <td>[4 skinless, boneless chicken breast halves , ...</td>\n",
       "      <td>[Place the chicken, butter, soup, and onion in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Awesome Slow Cooker Pot Roast</td>\n",
       "      <td>[2 (10.75 ounce) cans condensed cream of mushr...</td>\n",
       "      <td>[In a slow cooker, mix cream of mushroom soup,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brown Sugar Meatloaf</td>\n",
       "      <td>[1/2 cup packed brown sugar , 1/2 cup ketchup ...</td>\n",
       "      <td>[Preheat oven to 350 degrees F (175 degrees C)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Best Chocolate Chip Cookies</td>\n",
       "      <td>[1 cup butter, softened , 1 cup white sugar , ...</td>\n",
       "      <td>[Preheat oven to 350 degrees F (175 degrees C)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Title  \\\n",
       "0  Slow Cooker Chicken and Dumplings   \n",
       "1      Awesome Slow Cooker Pot Roast   \n",
       "2               Brown Sugar Meatloaf   \n",
       "3        Best Chocolate Chip Cookies   \n",
       "\n",
       "                                         Ingredients  \\\n",
       "0  [4 skinless, boneless chicken breast halves , ...   \n",
       "1  [2 (10.75 ounce) cans condensed cream of mushr...   \n",
       "2  [1/2 cup packed brown sugar , 1/2 cup ketchup ...   \n",
       "3  [1 cup butter, softened , 1 cup white sugar , ...   \n",
       "\n",
       "                                        Instructions  \n",
       "0  [Place the chicken, butter, soup, and onion in...  \n",
       "1  [In a slow cooker, mix cream of mushroom soup,...  \n",
       "2  [Preheat oven to 350 degrees F (175 degrees C)...  \n",
       "3  [Preheat oven to 350 degrees F (175 degrees C)...  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['Title'] = Title\n",
    "Data['Ingredients'] = Ingredients\n",
    "Data['Instructions'] = Instructions\n",
    "Data[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some help from: [TowardsDataScience](https://towardsdatascience.com/text-generation-with-python-and-gpt-2-1fecbff1635b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence\n",
    "Here we define an arbitrary recipe string sequence to initially train and test our model with. This text will be improved when we have acquired some data, but this will do for initial development:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'placeholder'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = \"\"\"Ingredients: 3 tomatoes, garlic, spaghetti, Spanish chili's, cheese. Boil the pasta, then rinse. Cut chili's and mince garlic, add to pan\n",
    "Add tomatoes and the pasta, and grate a generous amount of cheese\n",
    "Serve with a touch of basil and a glass of good red wine\n",
    "           \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Allemaal van Sergio\n",
    "\n",
    "\"\"\"\n",
    "seqdata = []\n",
    "print(len(data3.keys()))\n",
    "count = 0\n",
    "for key, value in data3.items():\n",
    "    check = False\n",
    "    \n",
    "    #check whether it has the key ingredients\n",
    "    if \"ingredients\" in value.keys():\n",
    "        check = True\n",
    "    \n",
    "    #amount of recipes you want\n",
    "    if count <= 800 and check:\n",
    "        count = count +1\n",
    "        listingr = \"ingredients: \"\n",
    "        for i in value[\"ingredients\"]:\n",
    "            listingr = listingr + i + \" \"\n",
    "        if type(value[\"instructions\"]) == str:\n",
    "            r = listingr + value[\"instructions\"]\n",
    "            seqdata.append(r)\n",
    "            continue\n",
    "\n",
    "print(type(seqdata))\n",
    "#first two recipes\n",
    "print(seqdata[2])\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"placeholder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use the pretrained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT2's development can be found at: Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2018). Language Models are Unsupervised Multitask Learners. 24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'placeholder'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenizing the inputs\n",
    "#print(seqdata[1])\n",
    "inputs = tokenizer.encode(sequence, return_tensors='pt')\n",
    "#print(inputs)\n",
    "\n",
    "\n",
    "\n",
    "#Allemaal van Sergio\n",
    "\n",
    "\"\"\"\n",
    "tokenizer.pad_token = \"tokenizer.eos_token\"\n",
    "#padding and truncation to make all tensors same size.\n",
    "#encoded_choices = [tokenizer.encode(s) for s in seqdata]\n",
    "input_ids = []\n",
    "\n",
    "for i in seqdata:\n",
    "    input_ids.append(tokenizer.encode(i, return_tensors='pt', padding=True,truncation = True))\n",
    "\n",
    "print(input_ids) #input Ids is now list with tensors of each datapoint\n",
    "\n",
    "\"\"\"\n",
    "'placeholder'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we generate a recipe based on the encoded cooking texts that we fed the model above. We will use the model.generate() function to do this.\n",
    "Model.generate() has a lot of different variable options worth looking at, which can be used to optimize our model. For instance, the temperature variable can be tweaked between 0-5 for increased randomness, we can try no_repeat_ngram_size=2 to prevent repitition, or tweak top_k. We'll just have to play around a bit and see what works best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# We set the output length to 500 tokens\n",
    "EncodedRecipe = model.generate(inputs, max_length=500, do_sample=True)\n",
    "\n",
    "\n",
    "\n",
    "#Allemaal van Sergio\n",
    "\"\"\"\n",
    "#this only works only when you pick a specific tensor of input ids and not for all\n",
    "EncodedRecipe2 = model.generate(input_ids[2], max_length=1024, do_sample=True)\n",
    "print(EncodedRecipe2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Decode the tokenized outputs\n",
    "#recipe = tokenizer.decode(EncodedRecipe[0], skip_special_tokens=True)\n",
    "#We filter out the initial context sequence given to the model (258 characters)\n",
    "#print(recipe[259:])\n",
    "\n",
    "#Decode the tokenized outputs\n",
    "recipe = tokenizer.decode(EncodedRecipe[0], skip_special_tokens=True)\n",
    "#We filter out the initial context sequence given to the model (258 characters)\n",
    "print(len(recipe))\n",
    "print(recipe) #part of it original and part of it the same as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this repetition in this recipe is a great example of why we may have to try ways to prevent repetition in the model generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save generated recipes as .txt files:\n",
    "def save(recipe, filename):\n",
    "    text_file = open(filename, \"w\", encoding = 'utf8')\n",
    "    n = text_file.write(recipe)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the previously generated recipe:\n",
    "save(recipe[259:], 'FirstRecipe.txt')\n",
    "#Note: I am leaving out the 259 context characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n",
    "I had some inspiration for a potentially novel approach to take this model in: reinforcement learning. While the current model already performs relatively well on creating sensical recipe texts, the model still has absolutely no idea of 'taste', and which ingredients work well together and which don't. Therefore, it may be desireable to give the model some sort of loss function score on its recipes so it knows which recipes are good and which are bad, so the model can improve.\n",
    "We could do this by either reading the recipes and rating them on how good we think they might be, or maybe even try to cook some of the things our model suggests and see how well they work, so we can help the model improve. While this may take some time and make the coding harder, it could be really cool to have a recipe generation model trained and optimized on actual real-world cooking, and I can hardly imagine anyone has ever done something like that before. Who knows, maybe the model will actually end up becoming a really good cook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contributors: Emilia, Sergio, Tim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
