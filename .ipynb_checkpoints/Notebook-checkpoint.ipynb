{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining Group Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative idea: maybe make model input a list of ingredients (based on what you have at home), and then let the model generate a reicpe for you based on the ingredients you have available\n",
    "\n",
    "if that is the route we take, we can alternatively make another function that generates random lists of ingredients, and then feed the random ingredients to the recipe maker based on ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#https://eightportions.com/datasets/Recipes/#fn:1 where the data is from\n",
    "#scraped from \n",
    "#Foodnetwork.com, Epicurious.com, Allrecipes.com by Ryan Lee\n",
    "\n",
    "allr = open('recipes_raw_nosource_ar.json')\n",
    "epi = open('recipes_raw_nosource_epi.json')\n",
    "food = open('recipes_raw_nosource_fn.json')\n",
    "\n",
    "data1 = json.load(allr) #data 1 has a lot of the word ADVERTISEMENT\n",
    "data2 = json.load(epi) #data 2 looks good\n",
    "data3 = json.load(food) #this too\n",
    "\n",
    "#this load module loads the data into a dictionary \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammie Hamblet's Deviled Crab\n"
     ]
    }
   ],
   "source": [
    "#Example of the double dict structue of the .json files:\n",
    "print(data3[\"p3pKOD6jIHEcjf20CCXohP8uqkG5dGi\"]['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, we really have to look at getting all of the recipes into the same format: so the model will learn to recognize the format, and start to use it.\n",
    "\n",
    "Our current recipes are in a dict of list of dict form, where the first dict contains the recipe code as key and the recipe as value, and the recipe value itself is a list of dicts in the shape [{instructions: value}, {ingredients: value}, {instructions:value}, {title:value}]. This needs to be processed to be more workable\n",
    "\n",
    "\n",
    "We start by defining a list of all the recipe codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = list(data1.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use this to display a specific recipe by iterating over its respective code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Banana Banana Bread',\n",
       " 'ingredients': ['2 cups all-purpose flour ADVERTISEMENT',\n",
       "  '1 teaspoon baking soda ADVERTISEMENT',\n",
       "  '1/4 teaspoon salt ADVERTISEMENT',\n",
       "  '1/2 cup butter ADVERTISEMENT',\n",
       "  '3/4 cup brown sugar ADVERTISEMENT',\n",
       "  '2 eggs, beaten ADVERTISEMENT',\n",
       "  '2 1/3 cups mashed overripe bananas ADVERTISEMENT',\n",
       "  'ADVERTISEMENT'],\n",
       " 'instructions': 'Preheat oven to 350 degrees F (175 degrees C). Lightly grease a 9x5 inch loaf pan.\\nIn a large bowl, combine flour, baking soda and salt. In a separate bowl, cream together butter and brown sugar. Stir in eggs and mashed bananas until well blended. Stir banana mixture into flour mixture; stir just to moisten. Pour batter into prepared loaf pan.\\nBake in preheated oven for 60 to 65 minutes, until a toothpick inserted into center of the loaf comes out clean. Let bread cool in pan for 10 minutes, then turn out onto a wire rack.\\n',\n",
       " 'picture_link': 'jRnWGDXDdyOg3rta4/HVAR2rD19XubC'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1[codes[5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some help from: [TowardsDataScience](https://towardsdatascience.com/text-generation-with-python-and-gpt-2-1fecbff1635b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence\n",
    "Here we define an arbitrary recipe string sequence to initially train and test our model with. This text will be improved when we have acquired some data, but this will do for initial development:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'placeholder'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = \"\"\"Ingredients: 3 tomatoes, garlic, spaghetti, Spanish chili's, cheese. Boil the pasta, then rinse. Cut chili's and mince garlic, add to pan\n",
    "Add tomatoes and the pasta, and grate a generous amount of cheese\n",
    "Serve with a touch of basil and a glass of good red wine\n",
    "           \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Allemaal van Sergio\n",
    "\n",
    "\"\"\"\n",
    "seqdata = []\n",
    "print(len(data3.keys()))\n",
    "count = 0\n",
    "for key, value in data3.items():\n",
    "    check = False\n",
    "    \n",
    "    #check whether it has the key ingredients\n",
    "    if \"ingredients\" in value.keys():\n",
    "        check = True\n",
    "    \n",
    "    #amount of recipes you want\n",
    "    if count <= 800 and check:\n",
    "        count = count +1\n",
    "        listingr = \"ingredients: \"\n",
    "        for i in value[\"ingredients\"]:\n",
    "            listingr = listingr + i + \" \"\n",
    "        if type(value[\"instructions\"]) == str:\n",
    "            r = listingr + value[\"instructions\"]\n",
    "            seqdata.append(r)\n",
    "            continue\n",
    "\n",
    "print(type(seqdata))\n",
    "#first two recipes\n",
    "print(seqdata[2])\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"placeholder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use the pretrained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT2's development can be found at: Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2018). Language Models are Unsupervised Multitask Learners. 24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'placeholder'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenizing the inputs\n",
    "#print(seqdata[1])\n",
    "inputs = tokenizer.encode(sequence, return_tensors='pt')\n",
    "#print(inputs)\n",
    "\n",
    "\n",
    "\n",
    "#Allemaal van Sergio\n",
    "\n",
    "\"\"\"\n",
    "tokenizer.pad_token = \"tokenizer.eos_token\"\n",
    "#padding and truncation to make all tensors same size.\n",
    "#encoded_choices = [tokenizer.encode(s) for s in seqdata]\n",
    "input_ids = []\n",
    "\n",
    "for i in seqdata:\n",
    "    input_ids.append(tokenizer.encode(i, return_tensors='pt', padding=True,truncation = True))\n",
    "\n",
    "print(input_ids) #input Ids is now list with tensors of each datapoint\n",
    "\n",
    "\"\"\"\n",
    "'placeholder'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we generate a recipe based on the encoded cooking texts that we fed the model above. We will use the model.generate() function to do this.\n",
    "Model.generate() has a lot of different variable options worth looking at, which can be used to optimize our model. For instance, the temperature variable can be tweaked between 0-5 for increased randomness, we can try no_repeat_ngram_size=2 to prevent repitition, or tweak top_k. We'll just have to play around a bit and see what works best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# We set the output length to 500 tokens\n",
    "EncodedRecipe = model.generate(inputs, max_length=500, do_sample=True)\n",
    "\n",
    "\n",
    "\n",
    "#Allemaal van Sergio\n",
    "\"\"\"\n",
    "#this only works only when you pick a specific tensor of input ids and not for all\n",
    "EncodedRecipe2 = model.generate(input_ids[2], max_length=1024, do_sample=True)\n",
    "print(EncodedRecipe2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Decode the tokenized outputs\n",
    "#recipe = tokenizer.decode(EncodedRecipe[0], skip_special_tokens=True)\n",
    "#We filter out the initial context sequence given to the model (258 characters)\n",
    "#print(recipe[259:])\n",
    "\n",
    "#Decode the tokenized outputs\n",
    "recipe = tokenizer.decode(EncodedRecipe[0], skip_special_tokens=True)\n",
    "#We filter out the initial context sequence given to the model (258 characters)\n",
    "print(len(recipe))\n",
    "print(recipe) #part of it original and part of it the same as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this repetition in this recipe is a great example of why we may have to try ways to prevent repetition in the model generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save generated recipes as .txt files:\n",
    "def save(recipe, filename):\n",
    "    text_file = open(filename, \"w\", encoding = 'utf8')\n",
    "    n = text_file.write(recipe)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the previously generated recipe:\n",
    "save(recipe[259:], 'FirstRecipe.txt')\n",
    "#Note: I am leaving out the 259 context characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n",
    "I had some inspiration for a potentially novel approach to take this model in: reinforcement learning. While the current model already performs relatively well on creating sensical recipe texts, the model still has absolutely no idea of 'taste', and which ingredients work well together and which don't. Therefore, it may be desireable to give the model some sort of loss function score on its recipes so it knows which recipes are good and which are bad, so the model can improve.\n",
    "We could do this by either reading the recipes and rating them on how good we think they might be, or maybe even try to cook some of the things our model suggests and see how well they work, so we can help the model improve. While this may take some time and make the coding harder, it could be really cool to have a recipe generation model trained and optimized on actual real-world cooking, and I can hardly imagine anyone has ever done something like that before. Who knows, maybe the model will actually end up becoming a really good cook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contributors: Emilia, Sergio, Tim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
