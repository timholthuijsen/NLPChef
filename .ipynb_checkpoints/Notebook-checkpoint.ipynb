{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining Group Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative idea: maybe make model input a list of ingredients (based on what you have at home), and then let the model generate a reicpe for you based on the ingredients you have available\n",
    "\n",
    "if that is the route we take, we can alternatively make another function that generates random lists of ingredients, and then feed the random ingredients to the recipe maker based on ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#https://eightportions.com/datasets/Recipes/#fn:1 where the data is from\n",
    "#scraped from \n",
    "#Foodnetwork.com, Epicurious.com, Allrecipes.com by Ryan Lee\n",
    "\n",
    "allr = open('recipes_raw_nosource_ar.json')\n",
    "epi = open('recipes_raw_nosource_epi.json')\n",
    "food = open('recipes_raw_nosource_fn.json')\n",
    "\n",
    "data1 = json.load(allr) #data 1 has a lot of the word ADVERTISEMENT\n",
    "data2 = json.load(epi) #data 2 looks good\n",
    "data3 = json.load(food) #this too\n",
    "\n",
    "#this load module loads the data into a dictionary \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammie Hamblet's Deviled Crab\n"
     ]
    }
   ],
   "source": [
    "#Example of the double dict structue of the .json files:\n",
    "print(data3[\"p3pKOD6jIHEcjf20CCXohP8uqkG5dGi\"]['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some help from [coursera](https://www.coursera.org/projects/generating-new-recipes-python)\n",
    "\n",
    "Our current recipes are in a dict of list of dict form, where the first dict contains the recipe code as key and the recipe as value, and the recipe value itself is a list of dicts in the shape [{title: value}, {ingredients: value}, {instructions:value}]. This needs to be processed to be more workable\n",
    "\n",
    "\n",
    "We start by defining a list of all the recipe codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first dataset contains 39802 recipes\n",
      "in total, we have 125164 recipes\n"
     ]
    }
   ],
   "source": [
    "#creating lists of keys\n",
    "codes1 = list(data1.keys())\n",
    "codes2 = list(data2.keys())\n",
    "codes3 = list(data3.keys())\n",
    "\n",
    "#and doing some data exploration\n",
    "NumRecipes = len(codes1) + len(codes2) + len(codes3)\n",
    "print('The first dataset contains', len(codes1), 'recipes')\n",
    "print('in total, we have', NumRecipes, 'recipes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Pandas dataframe\n",
    "The dict of lists of dicts format is rather annoying to work with for the modeling tasks we want to perform on it. Therefore, we convert everything nicely into our own pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data has been added to the lists succesfully!\n"
     ]
    }
   ],
   "source": [
    "#Create dataframe\n",
    "Data = pd.DataFrame()\n",
    "\n",
    "#initializing empty lists which we'll add to the dataframe\n",
    "Title = []\n",
    "Ingredients = []\n",
    "Instructions = []\n",
    "\n",
    "#a for-loop to put all the required data in the lists\n",
    "datasets = [data1, data2, data3]\n",
    "\n",
    "for data in datasets:\n",
    "    for _, val in data.items():\n",
    "        #We occasionally get keyerrors due to corrupted data\n",
    "        #so a try-except is added\n",
    "        try:\n",
    "            Title.append(val['title'])\n",
    "            Ingredients.append([\n",
    "                #And we remove the random ADVERTISEMENT clutter\n",
    "                ingredient.replace(\n",
    "                'ADVERTISEMENT', '') for ingredient in val['ingredients']])\n",
    "            Instructions.append([str(\n",
    "                val['instructions']).replace('ADVERTISEMENT','').replace('\\n', ' ')])                      \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "#Quick check to see if it worked\n",
    "if len(Title) == len(Ingredients) and len(Title) == len(Instructions):\n",
    "    print(\"All data has been added to the lists succesfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During this transformation, 517 empty values have been removed\n",
      "We now have 124647 recipes\n"
     ]
    }
   ],
   "source": [
    "print(\"During this transformation,\", NumRecipes - len(Title), \"empty values have been removed\")\n",
    "print(\"We now have\", len(Title), \"recipes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier, we noticed that the first dataset contained a lot of random ADVERTISEMENT strings scattered around. This clutter has also been removed during the list comprehensions used above\n",
    "#### Adding data to dataframe\n",
    "We can now add all of the data we just created and finish up the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Ingredients</th>\n",
       "      <th>Instructions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Slow Cooker Chicken and Dumplings</td>\n",
       "      <td>[4 skinless, boneless chicken breast halves , ...</td>\n",
       "      <td>[Place the chicken, butter, soup, and onion in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Awesome Slow Cooker Pot Roast</td>\n",
       "      <td>[2 (10.75 ounce) cans condensed cream of mushr...</td>\n",
       "      <td>[In a slow cooker, mix cream of mushroom soup,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brown Sugar Meatloaf</td>\n",
       "      <td>[1/2 cup packed brown sugar , 1/2 cup ketchup ...</td>\n",
       "      <td>[Preheat oven to 350 degrees F (175 degrees C)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Best Chocolate Chip Cookies</td>\n",
       "      <td>[1 cup butter, softened , 1 cup white sugar , ...</td>\n",
       "      <td>[Preheat oven to 350 degrees F (175 degrees C)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Title  \\\n",
       "0  Slow Cooker Chicken and Dumplings   \n",
       "1      Awesome Slow Cooker Pot Roast   \n",
       "2               Brown Sugar Meatloaf   \n",
       "3        Best Chocolate Chip Cookies   \n",
       "\n",
       "                                         Ingredients  \\\n",
       "0  [4 skinless, boneless chicken breast halves , ...   \n",
       "1  [2 (10.75 ounce) cans condensed cream of mushr...   \n",
       "2  [1/2 cup packed brown sugar , 1/2 cup ketchup ...   \n",
       "3  [1 cup butter, softened , 1 cup white sugar , ...   \n",
       "\n",
       "                                        Instructions  \n",
       "0  [Place the chicken, butter, soup, and onion in...  \n",
       "1  [In a slow cooker, mix cream of mushroom soup,...  \n",
       "2  [Preheat oven to 350 degrees F (175 degrees C)...  \n",
       "3  [Preheat oven to 350 degrees F (175 degrees C)...  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['Title'] = Title\n",
    "Data['Ingredients'] = Ingredients\n",
    "Data['Instructions'] = Instructions\n",
    "Data[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe Recognition\n",
    "With some help from: [TowardsDataScience](https://towardsdatascience.com/text-generation-with-python-and-gpt-2-1fecbff1635b), [StackExchange](https://datascience.stackexchange.com/questions/66394/how-does-bert-and-gpt-2-encoding-deal-with-token-such-as-startoftext-s), and [Coursera](https://www.coursera.org/projects/generating-new-recipes-python) and [TowardsDataScience2](https://towardsdatascience.com/train-gpt-2-in-your-own-language-fc6ad4d60171)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data processed, we want to use the database's structure to train our GPT-2 model. Specifically, we want it to learn to recognize the following structure:\n",
    "\n",
    "Ingredients: <br/> ingredient1 <br/> ingredient2 <br/> ingredient3 <br/><br/>\n",
    "Instructions: <br/> some text explaining instructions\n",
    "\n",
    "### Train/Test split\n",
    "In order to train our model towards this structure, we first create a training and validation set. We take 50% of the data as a test set, and 50% as validation set. We use such a small percentage for training since otherwise the dataset is simply too large to handle, and this model already takes hours to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GPT-2 special end of document token\n",
    "special_token = ' <|endoftext|> '\n",
    "\n",
    "#Create a dataframe column that combines ingredients and instructions\n",
    "Data['combined'] = ' \\n Ingredients: \\n ' + Data.Ingredients.str.join(' \\n ') + \\\n",
    "' \\n Instructions: \\n ' +Data.Instructions.str.join(' \\n ') + special_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(Data['Title'])\n",
    "train = Data[:int(0.1*length)].combined.values\n",
    "test = Data[int(0.9*length):].combined.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12464"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write training and test data to a file\n",
    "with open('TrainingData.txt','w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(train))\n",
    "with open('TestData.txt','w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "Now that we have the test and training datasets, we can train our model on this specific data. This task, however, is rather complicated: and cannot be performed solely in this notebook. Rather, we upload the TrainingData.txt to Google Colab. Then, we use the [run_lm_finetuning.py](https://github.com/alontalmor/pytorch-transformers/blob/master/examples/run_lm_finetuning.py) script from huggingface and define a bashscript run_experiments.sh to run it and to start training our model on the defined training data. run_lm_finetuning.py contains a number of errors, specifically with importing WarmupLinearSchedule from transformers. Therefore, we define our own WarmupLinearSchedule in the run_lm_finetuning.py, and use run_experiments.sh to train the custom model on our recently created training data. \n",
    "\n",
    "The trained model itself is unfortunately too large to upload to github, but the .sh and .py scripts used to train and create this custom model are attached with this assignment repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "#using the models trained on our dataset:\n",
    "tokenizer = AutoTokenizer.from_pretrained('LittleChef/')\n",
    "model = AutoModelForCausalLM.from_pretrained('LittleChef/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is where I am working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformers;'...\n",
      "fatal: unable to access 'https://github.com/huggingface/transformers;/': The requested URL returned error: 400\n",
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transformers;\n",
    "!cd transformers; pip3 install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'WarmupLinearSchedule' from 'transformers' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-193-b001b9640896>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWarmupLinearSchedule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'WarmupLinearSchedule' from 'transformers' (unknown location)"
     ]
    }
   ],
   "source": [
    "from transformers import WarmupLinearSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\timho\\anaconda3\\lib\\site-packages (1.19.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\timho\\anaconda3\\lib\\site-packages (1.19.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory â€˜experimentsâ€™: File exists\n",
      "Traceback (most recent call last):\n",
      "  File \"run_lm_finetuning.py\", line 33, in <module>\n",
      "    import numpy as np\n",
      "ModuleNotFoundError: No module named 'numpy'\n"
     ]
    }
   ],
   "source": [
    "!pip3 install numpy\n",
    "!pip install numpy\n",
    "!bash run_experiments.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "TrainSize = int(0.8 * len(Data['Title']))\n",
    "TrainTimes = []\n",
    "TestTimes = []\n",
    "for i in range(len(Data['Title'])):\n",
    "    TestTimes.append(i)\n",
    "for i in range(TrainSize):\n",
    "    item = random.choice(TestTimes)\n",
    "    TestTimes.remove(item)\n",
    "    TrainTimes.append(item)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([83810, 14592,  3278, 97199, 36050, 32100, 29258, 18291, 96537,\\n            13435,\\n            ...\\n            45099, 44008,  5012,  8575,  1697, 10614,   550, 54057, 70941,\\n            86657],\\n           dtype='int64', length=99717)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-169-5732adc215ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTrainTimes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombined\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2906\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2907\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2908\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2909\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2910\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1297\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1298\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m             \u001b[1;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Int64Index([83810, 14592,  3278, 97199, 36050, 32100, 29258, 18291, 96537,\\n            13435,\\n            ...\\n            45099, 44008,  5012,  8575,  1697, 10614,   550, 54057, 70941,\\n            86657],\\n           dtype='int64', length=99717)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "train = Data[TrainTimes].loc.combined.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first add some readability for the GPT-2 model to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timho\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\timho\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\timho\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\timho\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\timho\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\timho\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\timho\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\timho\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\timho\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\timho\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\timho\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\timho\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'save_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-160-f326d6af143f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGPT2Config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTFGPT2LMHeadModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# loading tokenizer from the saved model path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m tokenizer.add_special_tokens({\n\u001b[0;32m      6\u001b[0m   \u001b[1;34m\"eos_token\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"</s>\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'save_path' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import GPT2Config, TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "# loading tokenizer from the saved model path\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(save_path)\n",
    "tokenizer.add_special_tokens({\n",
    "  \"eos_token\": \"</s>\",\n",
    "  \"bos_token\": \"<s>\",\n",
    "  \"unk_token\": \"<unk>\",\n",
    "  \"pad_token\": \"<pad>\",\n",
    "  \"mask_token\": \"<mask>\"\n",
    "})\n",
    "# creating the configurations from which the model can be made\n",
    "config = GPT2Config(\n",
    "  vocab_size=tokenizer.vocab_size,\n",
    "  bos_token_id=tokenizer.bos_token_id,\n",
    "  eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "# creating the model\n",
    "model = TFGPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use the pretrained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT2's development can be found at: Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2018). Language Models are Unsupervised Multitask Learners. 24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  220,   198, 33474,    25,   220,   198,   604,  4168,  1203,    11,\n",
       "          5351,  5321,  9015,  9296, 37192,   220,   220,   198,   362, 30064,\n",
       "          9215,   220,   220,   198,   362,   357,   940,    13,  2425, 25799,\n",
       "             8, 23916, 38784,  8566,   286,  9015, 17141,   220,   220,   198,\n",
       "           352, 21670,    11, 32566, 48898,   220,   220,   198,   362,   357,\n",
       "           940, 25799,     8, 10392, 19866,   515, 36228,  5013, 15756,    11,\n",
       "         12445,   656,  5207,   220,   220,   198,   220,   220,   198,   220,\n",
       "           198, 27759,    25,   220,   198]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = Data['combined'][0].split('Instructions')[0] + '\\n Instructions: \\n'\n",
    "\n",
    "encoded = tokenizer.encode(sequence, add_special_tokens = False,\n",
    "                           return_tensors='pt')\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n Ingredients: \\n 4 skinless, boneless chicken breast halves  \\n 2 tablespoons butter  \\n 2 (10.75 ounce) cans condensed cream of chicken soup  \\n 1 onion, finely diced  \\n 2 (10 ounce) packages refrigerated biscuit dough, torn into pieces  \\n  \\n '"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['combined'][0].split('Instructions')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning\n",
    "See [Keskar et al.](https://arxiv.org/pdf/1909.05858.pdf) for more information on repetition penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "modelgenerated = model.generate(input_ids = encoded, max_length = 700,\n",
    "                                temperature = 0.9,\n",
    "                                top_k = 20,\n",
    "                                do_sample = True,\n",
    "                               repetition_penalty = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Ingredients: \n",
      " 4 skinless, boneless chicken breast halves  \n",
      " 2 tablespoons butter  \n",
      " 2 (10.75 ounce) cans condensed cream of chicken soup  \n",
      " 1 onion, finely diced  \n",
      " 2 (10 ounce) packages refrigerated biscuit dough, torn into pieces  \n",
      "  \n",
      " \n",
      " Instructions: \n",
      " 1. Add the chicken to an oiled large bowl.  While the chicken is cooking, stir together the butter and cream of chicken.  Mix well.  Add the chicken to the hot pan of a stand mixer over high speed.  Pour about 1 tablespoon of the cream of chicken.  I usually give about 1 tablespoon of the cream of chicken to about 5 people.  When the chicken is done mixing, use a spoon to scoop the mixture out as quickly as you can.  When the chicken is set aside in the fridge for about 5 minutes, then stir in the milk.  It is best if you get the chicken to a nice crispy crust by using a fork, but that will take a minute or two and it will go into the fridge. 2. Meanwhile,  mix the chicken with milk and the cheese (not the eggs) and sprinkle generously with salt.  You want the mixture to be very fluffy, but not too soft. 3. Once the egg and milk are combined, place the chicken into the fridge.  Use the spatula, or a small knife to cut the mixture into chunks until they are almost thickened to about 2 cm. 4. Once the egg and milk are combined, fold in the egg mixture.  Make a smooth layer of breadcrumbs. 5. While the egg mixture is in the fridge, cut each egg into 4 equal pieces, or two pieces at a time.  Cut the dough into 4 equal pieces, or 2 pieces at a time and cut out one piece each into about 1 cm cubes. Then, place each piece together on a small baking sheet. 6. Once the egg and milk are combined, fold in the butter and buttercream to make a thin layer. 7. If you have a little extra butter on hand,  use a small spoon to dip the egg mixture into a thin layer.  8. Once you have folded the buttercream, sprinkle in the egg mixture at the bottom and top of each layer. 9. Once the buttercream is in place, add the cream of chicken to the chicken mixture. 10. Cook for a minute, or until the chicken is golden brown on the outside.\n",
      "This recipe was made using 3/4 cup of milk.\n",
      "I've tried different brands of eggs, so you will need to try each separately.\n",
      "If you do not have a good or reliable way of measuring, please don't ask me if you can buy them all or ask for them on my blog.  I love experimenting with recipes, so you won't regret it.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(modelgenerated[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'placeholder'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenizing the inputs\n",
    "#print(seqdata[1])\n",
    "inputs = tokenizer.encode(sequence, return_tensors='pt')\n",
    "#print(inputs)\n",
    "\n",
    "\n",
    "\n",
    "#Allemaal van Sergio\n",
    "\n",
    "\"\"\"\n",
    "tokenizer.pad_token = \"tokenizer.eos_token\"\n",
    "#padding and truncation to make all tensors same size.\n",
    "#encoded_choices = [tokenizer.encode(s) for s in seqdata]\n",
    "input_ids = []\n",
    "\n",
    "for i in seqdata:\n",
    "    input_ids.append(tokenizer.encode(i, return_tensors='pt', padding=True,truncation = True))\n",
    "\n",
    "print(input_ids) #input Ids is now list with tensors of each datapoint\n",
    "\n",
    "\"\"\"\n",
    "'placeholder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'placeholder'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = \"\"\"Ingredients: 3 tomatoes, garlic, spaghetti, Spanish chili's, cheese. Boil the pasta, then rinse. Cut chili's and mince garlic, add to pan\n",
    "Add tomatoes and the pasta, and grate a generous amount of cheese\n",
    "Serve with a touch of basil and a glass of good red wine\n",
    "           \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Allemaal van Sergio\n",
    "\n",
    "\"\"\"\n",
    "seqdata = []\n",
    "print(len(data3.keys()))\n",
    "count = 0\n",
    "for key, value in data3.items():\n",
    "    check = False\n",
    "    \n",
    "    #check whether it has the key ingredients\n",
    "    if \"ingredients\" in value.keys():\n",
    "        check = True\n",
    "    \n",
    "    #amount of recipes you want\n",
    "    if count <= 800 and check:\n",
    "        count = count +1\n",
    "        listingr = \"ingredients: \"\n",
    "        for i in value[\"ingredients\"]:\n",
    "            listingr = listingr + i + \" \"\n",
    "        if type(value[\"instructions\"]) == str:\n",
    "            r = listingr + value[\"instructions\"]\n",
    "            seqdata.append(r)\n",
    "            continue\n",
    "\n",
    "print(type(seqdata))\n",
    "#first two recipes\n",
    "print(seqdata[2])\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"placeholder\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we generate a recipe based on the encoded cooking texts that we fed the model above. We will use the model.generate() function to do this.\n",
    "Model.generate() has a lot of different variable options worth looking at, which can be used to optimize our model. For instance, the temperature variable can be tweaked between 0-5 for increased randomness, we can try no_repeat_ngram_size=2 to prevent repitition, or tweak top_k. We'll just have to play around a bit and see what works best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# We set the output length to 500 tokens\n",
    "EncodedRecipe = model.generate(inputs, max_length=500, do_sample=True)\n",
    "\n",
    "\n",
    "\n",
    "#Allemaal van Sergio\n",
    "\"\"\"\n",
    "#this only works only when you pick a specific tensor of input ids and not for all\n",
    "EncodedRecipe2 = model.generate(input_ids[2], max_length=1024, do_sample=True)\n",
    "print(EncodedRecipe2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Decode the tokenized outputs\n",
    "#recipe = tokenizer.decode(EncodedRecipe[0], skip_special_tokens=True)\n",
    "#We filter out the initial context sequence given to the model (258 characters)\n",
    "#print(recipe[259:])\n",
    "\n",
    "#Decode the tokenized outputs\n",
    "recipe = tokenizer.decode(EncodedRecipe[0], skip_special_tokens=True)\n",
    "#We filter out the initial context sequence given to the model (258 characters)\n",
    "print(len(recipe))\n",
    "print(recipe) #part of it original and part of it the same as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this repetition in this recipe is a great example of why we may have to try ways to prevent repetition in the model generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save generated recipes as .txt files:\n",
    "def save(recipe, filename):\n",
    "    text_file = open(filename, \"w\", encoding = 'utf8')\n",
    "    n = text_file.write(recipe)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the previously generated recipe:\n",
    "save(recipe[259:], 'FirstRecipe.txt')\n",
    "#Note: I am leaving out the 259 context characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n",
    "I had some inspiration for a potentially novel approach to take this model in: reinforcement learning. While the current model already performs relatively well on creating sensical recipe texts, the model still has absolutely no idea of 'taste', and which ingredients work well together and which don't. Therefore, it may be desireable to give the model some sort of loss function score on its recipes so it knows which recipes are good and which are bad, so the model can improve.\n",
    "We could do this by either reading the recipes and rating them on how good we think they might be, or maybe even try to cook some of the things our model suggests and see how well they work, so we can help the model improve. While this may take some time and make the coding harder, it could be really cool to have a recipe generation model trained and optimized on actual real-world cooking, and I can hardly imagine anyone has ever done something like that before. Who knows, maybe the model will actually end up becoming a really good cook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contributors: Emilia, Sergio, Tim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
