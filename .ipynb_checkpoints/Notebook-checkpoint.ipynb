{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining Group Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative idea: maybe make model input a list of ingredients (based on what you have at home), and then let the model generate a reicpe for you based on the ingredients you have available\n",
    "\n",
    "if that is the route we take, we can alternatively make another function that generates random lists of ingredients, and then feed the random ingredients to the recipe maker based on ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial model:\n",
    "When we started this project, we initially had a very different modeling approach from our final version. In order to showcase our thought process, we start this analysis off by creating our initial recipe generation model from the pretrained GPT-2 model.\n",
    "\n",
    "with some help from [TowardsDataScience](https://towardsdatascience.com/text-generation-with-python-and-gpt-2-1fecbff1635b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "#Defining the model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "#Defining a context sequence\n",
    "sequence = 'Bake 5 eggs. Add them to the tomatoes. Boil the pasta.'\n",
    "#encoding input\n",
    "encoded = tokenizer.encode(sequence, return_tensors='pt')\n",
    "#Letting the model generate text based on the context\n",
    "output = model.generate(encoded, max_length=100, \n",
    "                        do_sample=True,\n",
    "                        temperature = 7.0,\n",
    "                        top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bake 5 eggs. Add them to the tomatoes. Boil the pasta. Put into a plastic plastic baking tric-ten (one-six). Cook till very paleo/dairy-sweet with milk fro thyst. Set aside for another 5-3 hours or until you add 2 tsp butter and a tablespoon salt and allow until the mixture begins a smooth dough, or add a tablespoon, 1 minute or more and you can just leave 1 to 3 tablespoons left on plate to set at\n"
     ]
    }
   ],
   "source": [
    "recipe = tokenizer.decode(output[0])\n",
    "print(recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#https://eightportions.com/datasets/Recipes/#fn:1 where the data is from\n",
    "#scraped from \n",
    "#Foodnetwork.com, Epicurious.com, Allrecipes.com by Ryan Lee\n",
    "\n",
    "allr = open('recipes_raw_nosource_ar.json')\n",
    "epi = open('recipes_raw_nosource_epi.json')\n",
    "food = open('recipes_raw_nosource_fn.json')\n",
    "\n",
    "data1 = json.load(allr) #data 1 has a lot of the word ADVERTISEMENT\n",
    "data2 = json.load(epi) #data 2 looks good\n",
    "data3 = json.load(food) #this too\n",
    "\n",
    "#this load module loads the data into a dictionary \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammie Hamblet's Deviled Crab\n"
     ]
    }
   ],
   "source": [
    "#Example of the double dict structue of the .json files:\n",
    "print(data3[\"p3pKOD6jIHEcjf20CCXohP8uqkG5dGi\"]['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some help from [coursera](https://www.coursera.org/projects/generating-new-recipes-python)\n",
    "\n",
    "Our current recipes are in a dict of list of dict form, where the first dict contains the recipe code as key and the recipe as value, and the recipe value itself is a list of dicts in the shape [{title: value}, {ingredients: value}, {instructions:value}]. This needs to be processed to be more workable\n",
    "\n",
    "\n",
    "We start by defining a list of all the recipe codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first dataset contains 39802 recipes\n",
      "in total, we have 125164 recipes\n"
     ]
    }
   ],
   "source": [
    "#creating lists of keys\n",
    "codes1 = list(data1.keys())\n",
    "codes2 = list(data2.keys())\n",
    "codes3 = list(data3.keys())\n",
    "\n",
    "#and doing some data exploration\n",
    "NumRecipes = len(codes1) + len(codes2) + len(codes3)\n",
    "print('The first dataset contains', len(codes1), 'recipes')\n",
    "print('in total, we have', NumRecipes, 'recipes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Pandas dataframe\n",
    "The dict of lists of dicts format is rather annoying to work with for the modeling tasks we want to perform on it. Therefore, we convert everything nicely into our own pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data has been added to the lists succesfully!\n"
     ]
    }
   ],
   "source": [
    "#Create dataframe\n",
    "Data = pd.DataFrame()\n",
    "\n",
    "#initializing empty lists which we'll add to the dataframe\n",
    "Title = []\n",
    "Ingredients = []\n",
    "Instructions = []\n",
    "\n",
    "#a for-loop to put all the required data in the lists\n",
    "datasets = [data1, data2, data3]\n",
    "\n",
    "for data in datasets:\n",
    "    for _, val in data.items():\n",
    "        #We occasionally get keyerrors due to corrupted data\n",
    "        #so a try-except is added\n",
    "        try:\n",
    "            Title.append(val['title'])\n",
    "            Ingredients.append([\n",
    "                #And we remove the random ADVERTISEMENT clutter\n",
    "                ingredient.replace(\n",
    "                'ADVERTISEMENT', '') for ingredient in val['ingredients']])\n",
    "            Instructions.append([str(\n",
    "                val['instructions']).replace('ADVERTISEMENT','').replace('\\n', ' ')])                      \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "#Quick check to see if it worked\n",
    "if len(Title) == len(Ingredients) and len(Title) == len(Instructions):\n",
    "    print(\"All data has been added to the lists succesfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During this transformation, 517 empty values have been removed\n",
      "We now have 124647 recipes\n"
     ]
    }
   ],
   "source": [
    "print(\"During this transformation,\", NumRecipes - len(Title), \"empty values have been removed\")\n",
    "print(\"We now have\", len(Title), \"recipes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier, we noticed that the first dataset contained a lot of random ADVERTISEMENT strings scattered around. This clutter has also been removed during the list comprehensions used above\n",
    "#### Adding data to dataframe\n",
    "We can now add all of the data we just created and finish up the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Ingredients</th>\n",
       "      <th>Instructions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Slow Cooker Chicken and Dumplings</td>\n",
       "      <td>[4 skinless, boneless chicken breast halves , ...</td>\n",
       "      <td>[Place the chicken, butter, soup, and onion in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Awesome Slow Cooker Pot Roast</td>\n",
       "      <td>[2 (10.75 ounce) cans condensed cream of mushr...</td>\n",
       "      <td>[In a slow cooker, mix cream of mushroom soup,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brown Sugar Meatloaf</td>\n",
       "      <td>[1/2 cup packed brown sugar , 1/2 cup ketchup ...</td>\n",
       "      <td>[Preheat oven to 350 degrees F (175 degrees C)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Best Chocolate Chip Cookies</td>\n",
       "      <td>[1 cup butter, softened , 1 cup white sugar , ...</td>\n",
       "      <td>[Preheat oven to 350 degrees F (175 degrees C)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Homemade Mac and Cheese Casserole</td>\n",
       "      <td>[8 ounces whole wheat rotini pasta , 3 cups fr...</td>\n",
       "      <td>[Preheat oven to 350 degrees F. Line a 2-quart...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Title  \\\n",
       "0  Slow Cooker Chicken and Dumplings   \n",
       "1      Awesome Slow Cooker Pot Roast   \n",
       "2               Brown Sugar Meatloaf   \n",
       "3        Best Chocolate Chip Cookies   \n",
       "4  Homemade Mac and Cheese Casserole   \n",
       "\n",
       "                                         Ingredients  \\\n",
       "0  [4 skinless, boneless chicken breast halves , ...   \n",
       "1  [2 (10.75 ounce) cans condensed cream of mushr...   \n",
       "2  [1/2 cup packed brown sugar , 1/2 cup ketchup ...   \n",
       "3  [1 cup butter, softened , 1 cup white sugar , ...   \n",
       "4  [8 ounces whole wheat rotini pasta , 3 cups fr...   \n",
       "\n",
       "                                        Instructions  \n",
       "0  [Place the chicken, butter, soup, and onion in...  \n",
       "1  [In a slow cooker, mix cream of mushroom soup,...  \n",
       "2  [Preheat oven to 350 degrees F (175 degrees C)...  \n",
       "3  [Preheat oven to 350 degrees F (175 degrees C)...  \n",
       "4  [Preheat oven to 350 degrees F. Line a 2-quart...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['Title'] = Title\n",
    "Data['Ingredients'] = Ingredients\n",
    "Data['Instructions'] = Instructions\n",
    "Data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe Recognition\n",
    "With some help from: [TowardsDataScience](https://towardsdatascience.com/text-generation-with-python-and-gpt-2-1fecbff1635b), [StackExchange](https://datascience.stackexchange.com/questions/66394/how-does-bert-and-gpt-2-encoding-deal-with-token-such-as-startoftext-s), and [Coursera](https://www.coursera.org/projects/generating-new-recipes-python) and [TowardsDataScience2](https://towardsdatascience.com/train-gpt-2-in-your-own-language-fc6ad4d60171)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data processed, we want to use the database's structure to train our GPT-2 model. Specifically, we want it to learn to recognize the following structure:\n",
    "\n",
    "Ingredients: <br/> ingredient1 <br/> ingredient2 <br/> ingredient3 <br/><br/>\n",
    "Instructions: <br/> some text explaining instructions\n",
    "\n",
    "### Train/Test split\n",
    "In order to train our model towards this structure, we first create a training and validation set. We take 10% of the data as a traing set, and 50% as test set. We use such a small percentage for training since otherwise the dataset is simply too large to for the model training, and this model already takes hours to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GPT-2 special end of document token\n",
    "special_token = ' <|endoftext|> '\n",
    "\n",
    "#Create a dataframe column that combines ingredients and instructions\n",
    "Data['combined'] = ' \\n Ingredients: \\n ' + Data.Ingredients.str.join(' \\n ') + \\\n",
    "' \\n Instructions: \\n ' +Data.Instructions.str.join(' \\n ') + special_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(Data['Title'])\n",
    "train = Data[:int(0.1*length)].combined.values\n",
    "test = Data[int(0.9*length):].combined.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12464"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write training and test data to a file\n",
    "with open('TrainingData.txt','w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(train))\n",
    "with open('TestData.txt','w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "Now that we have the test and training datasets, we can train our model on this specific data. This task, however, is rather complicated: and cannot be performed solely in this notebook. Rather, we upload the TrainingData.txt to Google Colab. Then, we use the [run_lm_finetuning.py](https://github.com/alontalmor/pytorch-transformers/blob/master/examples/run_lm_finetuning.py) script from huggingface and define a bashscript run_experiments.sh to run it and to start training our model on the defined training data. run_lm_finetuning.py contains a number of errors, specifically with importing WarmupLinearSchedule from transformers. Therefore, we define our own WarmupLinearSchedule in the run_lm_finetuning.py, and use run_experiments.sh to train the custom model on our recently created training data. \n",
    "\n",
    "The trained model itself is unfortunately too large to upload to github, but the .sh and .py scripts used to train and create this custom model are attached with this assignment repository. 6 out of the 7 files of which our model consists do fit on github, and can be found in the /CustomModel directory. For the sake of inclusivity, our entire trained model is also available [here](https://1drv.ms/u/s!AlUeI82AcSLCo41KNfOUS5dTqT0tEQ?e=4Wyq21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the models trained on our dataset:\n",
    "tokenizer = AutoTokenizer.from_pretrained('CustomModel/')\n",
    "model = AutoModelForCausalLM.from_pretrained('CustomModel/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to test whether our trained model recognizes the Ingredients: etc. Instructions: etc. format.\n",
    "Therefore, we define a testsequence consisting of a list of ingredients from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsequence = test[100].split('Instructions')[0] + \" \\n Instructions:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Ingredients: \n",
      " 1/4 cup oil \n",
      " 6 medium onions, chopped \n",
      " 4 bell peppers, chopped \n",
      " 3 carrots, chopped \n",
      " 1 cup string beans, broken into pieces \n",
      " 3/4 cup peas \n",
      " 6 tomatoes, chopped \n",
      " 1/2 teaspoon black pepper \n",
      " 1 teaspoon dried thyme \n",
      " 4 cups medium grain brown rice, cooked (cold) \n",
      " 1/2 cup tomato paste \n",
      "  \n",
      " Instructions:\n"
     ]
    }
   ],
   "source": [
    "print(testsequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(testsequence, add_special_tokens = False,\n",
    "                           return_tensors='pt')\n",
    "\n",
    "modelgenerated = model.generate(input_ids = encoded, max_length = 700,\n",
    "                                temperature = 0.9,\n",
    "                                top_k = 20,\n",
    "                                do_sample = True,\n",
    "                               repetition_penalty = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Ingredients: \n",
      " 1/4 cup oil \n",
      " 6 medium onions, chopped \n",
      " 4 bell peppers, chopped \n",
      " 3 carrots, chopped \n",
      " 1 cup string beans, broken into pieces \n",
      " 3/4 cup peas \n",
      " 6 tomatoes, chopped \n",
      " 1/2 teaspoon black pepper \n",
      " 1 teaspoon dried thyme \n",
      " 4 cups medium grain brown rice, cooked (cold) \n",
      " 1/2 cup tomato paste \n",
      "  \n",
      " Instructions: \n",
      " Mix together the oil, oil, salt, and pepper in a large skillet over medium heat. Cook potatoes, then stir in the carrots and tomatoes, then bring to boil. Add beans and cook in the preheated oven until golden brown, about 5 minutes. Add beans, peas and peas. Cook until rice is tender and tender, about 3 minutes. Add tomatoes and stir in thyme, then add beans, peas, peas, and peas. Serve cold. Heat oil in a Dutch oven and cook potatoes through until golden brown. Place in a medium saucepan. Cook potatoes and carrots in the saucepan until softened, about 4 minutes. Add beans and bring to a boil. Remove from the heat, remove from the heat, and simmer until beans are tender, about 1 hour. Remove from the heat and set aside. Pour water directly into the boiling water using a spoon. Heat the potatoes in a preheated oven and set aside. Place the potatoes and carrots in the saucepan. Bake for 12 minutes or until potatoes are tender and brown on both sides, about 8 minutes each side. <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "for sequence in modelgenerated:\n",
    "    recipe = tokenizer.decode(sequence)\n",
    "    print(recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-26b13342fb78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrecipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelgenerated\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3087\u001b[0m             \u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3088\u001b[0m             \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3089\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3090\u001b[0m         )\n\u001b[0;32m   3091\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m             \u001b[0mtoken_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 507\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "recipe = tokenizer.decode(modelgenerated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is where I am working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/huggingface/transformers;\n",
    "!cd transformers; pip3 install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WarmupLinearSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install numpy\n",
    "!pip install numpy\n",
    "!bash run_experiments.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "TrainSize = int(0.8 * len(Data['Title']))\n",
    "TrainTimes = []\n",
    "TestTimes = []\n",
    "for i in range(len(Data['Title'])):\n",
    "    TestTimes.append(i)\n",
    "for i in range(TrainSize):\n",
    "    item = random.choice(TestTimes)\n",
    "    TestTimes.remove(item)\n",
    "    TrainTimes.append(item)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Data[TrainTimes].loc.combined.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning\n",
    "See [Keskar et al.](https://arxiv.org/pdf/1909.05858.pdf) for more information on repetition penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first add some readability for the GPT-2 model to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import GPT2Config, TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "# loading tokenizer from the saved model path\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(save_path)\n",
    "tokenizer.add_special_tokens({\n",
    "  \"eos_token\": \"</s>\",\n",
    "  \"bos_token\": \"<s>\",\n",
    "  \"unk_token\": \"<unk>\",\n",
    "  \"pad_token\": \"<pad>\",\n",
    "  \"mask_token\": \"<mask>\"\n",
    "})\n",
    "# creating the configurations from which the model can be made\n",
    "config = GPT2Config(\n",
    "  vocab_size=tokenizer.vocab_size,\n",
    "  bos_token_id=tokenizer.bos_token_id,\n",
    "  eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "# creating the model\n",
    "model = TFGPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT2's development can be found at: Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2018). Language Models are Unsupervised Multitask Learners. 24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = Data['combined'][0].split('Instructions')[0] + '\\n Instructions: \\n'\n",
    "\n",
    "encoded = tokenizer.encode(sequence, add_special_tokens = False,\n",
    "                           return_tensors='pt')\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(modelgenerated[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we generate a recipe based on the encoded cooking texts that we fed the model above. We will use the model.generate() function to do this.\n",
    "Model.generate() has a lot of different variable options worth looking at, which can be used to optimize our model. For instance, the temperature variable can be tweaked between 0-5 for increased randomness, we can try no_repeat_ngram_size=2 to prevent repitition, or tweak top_k. We'll just have to play around a bit and see what works best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# We set the output length to 500 tokens\n",
    "EncodedRecipe = model.generate(inputs, max_length=500, do_sample=True)\n",
    "\n",
    "\n",
    "\n",
    "#Allemaal van Sergio\n",
    "\"\"\"\n",
    "#this only works only when you pick a specific tensor of input ids and not for all\n",
    "EncodedRecipe2 = model.generate(input_ids[2], max_length=1024, do_sample=True)\n",
    "print(EncodedRecipe2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Decode the tokenized outputs\n",
    "#recipe = tokenizer.decode(EncodedRecipe[0], skip_special_tokens=True)\n",
    "#We filter out the initial context sequence given to the model (258 characters)\n",
    "#print(recipe[259:])\n",
    "\n",
    "#Decode the tokenized outputs\n",
    "recipe = tokenizer.decode(EncodedRecipe[0], skip_special_tokens=True)\n",
    "#We filter out the initial context sequence given to the model (258 characters)\n",
    "print(len(recipe))\n",
    "print(recipe) #part of it original and part of it the same as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this repetition in this recipe is a great example of why we may have to try ways to prevent repetition in the model generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save generated recipes as .txt files:\n",
    "def save(recipe, filename):\n",
    "    text_file = open(filename, \"w\", encoding = 'utf8')\n",
    "    n = text_file.write(recipe)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the previously generated recipe:\n",
    "save(recipe[259:], 'FirstRecipe.txt')\n",
    "#Note: I am leaving out the 259 context characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n",
    "I had some inspiration for a potentially novel approach to take this model in: reinforcement learning. While the current model already performs relatively well on creating sensical recipe texts, the model still has absolutely no idea of 'taste', and which ingredients work well together and which don't. Therefore, it may be desireable to give the model some sort of loss function score on its recipes so it knows which recipes are good and which are bad, so the model can improve.\n",
    "We could do this by either reading the recipes and rating them on how good we think they might be, or maybe even try to cook some of the things our model suggests and see how well they work, so we can help the model improve. While this may take some time and make the coding harder, it could be really cool to have a recipe generation model trained and optimized on actual real-world cooking, and I can hardly imagine anyone has ever done something like that before. Who knows, maybe the model will actually end up becoming a really good cook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contributors: Emilia, Sergio, Tim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
